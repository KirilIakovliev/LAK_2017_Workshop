{"cells":[{"cell_type":"markdown","source":["#Titanic Survivors Machine Learning Application\n\n#![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png) + ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png) + ![titanic](https://upload.wikimedia.org/wikipedia/commons/thumb/f/fd/RMS_Titanic_3.jpg/300px-RMS_Titanic_3.jpg)\n\n\n** This notebook covers: **\n* *Part 0: Import Libraries to Use for this Notebook*\n* *Part 1: Brief Introduction to Apache Spark*\n* *Part 2: Import Titanic Dataset as a Spark Dataframe*\n* *Part 3: Choosing Features and Cleaning Data*\n* *Part 4: Working with Categorical Features and Generating Training/Testing Datasets*\n* *Part 5: Train and Test a Logistic Regression Model*\n* *Part 6: Train and Test a Decision Tree Model*\n* *Part 7: Comparing the Logistic Regression Model to the Decision Tree Model*\n\n***\n\n# Part 0: Import Libraries to use for this Notebook"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col, udf\nfrom pyspark.sql.types import DoubleType, StringType, IntegerType\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer, VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["# Part 1: Brief Introduction to Apache Spark\n\n## What is Apache Spark?\nApache Spark is a parallel computing framework. \nIt has been designed for fast and easy parallelization of software by abstracting the complexities of parallel computing away from the user with an increasing set of APIs.\n\nThere are currently 2 main datatypes used in Apache Spark.\n\n* __RDD__ (Resiliant Distributed Dataset): An array like datastructure which can hold arbitrary objects within it. The RDD is distributed across a Spark cluster so that differnt nodes act on different data from the RDD. This follows the familiar map-reduce paradigm of Hadoop. This was the orignal main datastructure of Apache Spark.\n* __Dataframe__: A table like datastructure very similar to an Excel spreadsheet, an R dataframe, or a python-pandas dataframe.\n  \nCurrently, there are a few main components/libraries for Apache Spark.\n\n* __Core Apache Spark__:\n  * Main APIs for working with both RDDs and Dataframes\n* __Spark SQL__:\n  * APIs for working with Dataframes using SQL like syntax\n* __GraphX__:\n  * APIs for building parallel graph structures\n* __Spark Streaming__:\n  * APIs for dealing with streaming data\n* __MLib__:\n  * APIs for parallel machine learning algorithms\n  \n## Let's do some basic operations on a test Spark Dataframe"],"metadata":{}},{"cell_type":"code","source":["# Define some data\nd1 = (\"Nick\", 31, 70)\nd2 = (\"Shirin\", 30, 65)\nd3 = (\"Al\", 21, 65)\ndata = [d1, d2, d3]\n\n# Define the columns for the data\ncolumns = ['Name', 'Age', 'Height']\n\n# Create Dataframe with the list of tuples and the list of column names\ntest_dataframe = sqlContext.createDataFrame(data, columns)\ndisplay(test_dataframe)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["## Select columns from the Dataframe\nLet's create a new Dataframe that only contains the \"Name\" and \"Age\" columns"],"metadata":{}},{"cell_type":"code","source":["df = test_dataframe.select(col('Name'), col('Age'))\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["## Filter data from Dataframe\nLet's filter the Dataframe to only have rows with the \"Name\" column equal to \"Al\""],"metadata":{}},{"cell_type":"code","source":["df = test_dataframe.where(col('Name') == 'Al')\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["df = test_dataframe.filter(col('Name') == 'Al')\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["## Add a column to the Dataframe\nLet's create a new column called \"Type\" which uses the \"Age\" column to give a type of eitheer \"adult\" or \"child\""],"metadata":{}},{"cell_type":"code","source":["# Define the user defined function (UDF)\nage_height_udf = udf(lambda age: \"adult\" if age >21 else \"child\", StringType())\n\ndf = test_dataframe.withColumn(\"Type\", age_height_udf(test_dataframe['Age']))\n\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["## There is so much more! We have only scratched the surface\nWe have only gone over some of the most widely used functions for Spark Dataframes.\nThe office [Apache Spark page](http://spark.apache.org/) offers lots of great resources.\n\n* [Basic Example](http://spark.apache.org/examples.html)\n* [Spark SQL](http://spark.apache.org/sql/)\n* [Spark Streaming](http://spark.apache.org/streaming/)\n* [Spark Machine Learning](http://spark.apache.org/mllib/)\n* [Spark GraphX](http://spark.apache.org/graphx/)\n\n***\n\n# Part 2: Import Titanic Dataset as a Spark Dataframe\n\n## Create a table named \"titanic\" from the train.csv file from Kaggle\n\n\n\n## Dataset Definition\nTitanic dataset from [kaggle](https://www.kaggle.com/c/titanic/data).\n\n### Data Dictionary\n* survival - Survival (0 = No, 1 = Yes)\n* pclass - Ticket class\t(1 = 1st, 2 = 2nd, 3 = 3rd)\n* sex - Sex\t\n* Age - Age in years\t\n* sibsp - # of siblings / spouses aboard the Titanic\t\n* parch\t- # of parents / children aboard the Titanic\t\n* ticket - Ticket number\n* fare - Passenger fare\n* cabin - Cabin number\n* embarked - Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)\n\n#### Variable Notes\n* pclass: A proxy for socio-economic status (SES)\n  * 1st = Upper\n  * 2nd = Middle\n  * 3rd = Lower\n* age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n* sibsp: The dataset defines family relations in this way...\n  * Sibling = brother, sister, stepbrother, stepsister\n  * Spouse = husband, wife (mistresses and fiancÃ©s were ignored)\n* parch: The dataset defines family relations in this way...\n  * Parent = mother, father\n  * Child = daughter, son, stepdaughter, stepson\n  * Some children travelled only with a nanny, therefore parch=0 for them."],"metadata":{}},{"cell_type":"code","source":["# Read data from the newly created titanic table into a dataframe\ntitanic_df = sqlContext.sql(\"SELECT * FROM titanic\")\ndisplay(titanic_df)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["# Part 3: Choosing Features and Cleaning Data\n\n## What are we predicting?\nOur goal for this application will be to predict if a passenger survived the sinking of the Titanic or not given features about the passenger. Our label which we are trying to predict will be the __Survived__ column.\n\n## Feature Columns\n* __Pclass__: Categorical\n* __Sex__: Categorical\n* __Age__: Float\n* __SibSp__: Categorical\n* __Parch__: Categorical\n* __Fare__: Float\n* __Embarked__: Categorical\n\n## Cleaning Data Before Moving On\nThere are null values in the Titanic dataset for some columns we will be using. We will now select only columns we will be using as features or labels and then filter to only keep rows with non-null values. Our new filtered dataframe will be called __filtered_df__.\n\n__Note:__ In a real world application it may be desirable to transform null column values to non-null values depending on the context of the application and the data."],"metadata":{}},{"cell_type":"code","source":["# Select only the columns we will use for features or label\ntmp_df = titanic_df.select(col('Survived').cast(IntegerType()).alias('label'), \n                           col('Pclass').cast(StringType()).alias('Pclass'), \n                           col('Sex').cast(StringType()).alias('Sex'), \n                           col('Age').cast(DoubleType()).alias('Age'),\n                           col('SibSp').cast(StringType()).alias('SibSp'), \n                           col('Parch').cast(StringType()).alias('Parch'), \n                           col('Fare').cast(DoubleType()).alias('Fare'),\n                           col('Embarked').cast(StringType()).alias('Embarked')\n                          )\n\n# Filter any rows from tmp_df that have null values in any of their columns\ng = ((col('label').isNotNull()) & \n     (col('Pclass').isNotNull()) & \n     (col('Sex').isNotNull()) & \n     (col('Age').isNotNull()) & \n     (col('SibSp').isNotNull()) & \n     (col('Parch').isNotNull()) & \n     (col('Fare').isNotNull()) &\n     (col('Embarked').isNotNull())\n    )\nfiltered_df = tmp_df.where(g)\n\nprint(\"%d rows in original dataset\\n%d rows in filtered dataset\\n\\n%d rows filtered with null values\" % (titanic_df.count(), filtered_df.count(), titanic_df.count()-filtered_df.count()))"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["display(filtered_df)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["# Part 4: Working with Categorical Features and Generating Training/Testing Datasets\nCategorical features need special treatment since mathematically the ordering of categories is arbitrary.\nFor example, the __Sex__ column contains values (male and female) which could be converted to (0 and 1) but this is not good!\nBetter to use [one hot encoding](https://en.wikipedia.org/wiki/One-hot) which converts each feature into an _n_ dimensional vector where _n_ is the number of differnt categories for the feature.\n\nTherefore, for the __Sex__ column we can use one hot encoding\n\nmale   -> [0,1]\nfemale -> [1,0]\n\n## One hot encoding in Spark\nLet us now use Spark to apply one hot encoding on the __Sex__ column of data.\n\n### Step 1: Get index from categorical columns\nConvert variables of strings to an index. For example\n\ndata = ['male', 'male', 'female', 'male', 'female']\n\nwould become\n\nindexedData = [0, 0, 1, 0, 1]"],"metadata":{}},{"cell_type":"code","source":["# Create a StringIndexer object for the Sex column and create an indexed Sex_numeric column\nindexer = StringIndexer(inputCol=\"Sex\", outputCol=\"Sex_numeric\").fit(filtered_df)\n\n# Use new indexer object to transform the filtered_df dataframe, adding a new column to it\nindexed_df = indexer.transform(filtered_df)\n\n# Display only the Sex and Sex_numeric columns for clarity\ndisplay(indexed_df.select(col('Sex'), col('Sex_numeric')))"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["### Step 2: Get One Hot Encoding Vector for Indexed Categorical Data\n\nWe will now use the indexed categorical data to create a one hot encoding vector for the __Sex__ column features. In Spark, sparse vectors are used which only shows values if they are non-zero in a map type way. \n\nFor example:\n\n[0, 0, 1, 2] -> (0, 2, [2, 3], [1, 2]) -> (first_numeric_value, last_numeric_value, [non-zero_indicies], [non-zero_values])"],"metadata":{}},{"cell_type":"code","source":["# Create a OneHotEncoder object for the Sex_numeric column as Sex_vector column\nencoder = OneHotEncoder(inputCol=\"Sex_numeric\", outputCol=\"Sex_vector\")\n\n# Use new encoder object to transform the indexed_df dataframe, adding a new column to it\nencoded_df = encoder.transform(indexed_df)\n\n# Display only the Sex, Sex_numeric, and Sex_vector columns for clarity\ndisplay(encoded_df.select(col('Sex'), col('Sex_numeric'), col('Sex_vector')))"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["### Step 3: Setup a Pipeline to Encode all Categorical Variables\nSpark offers a [Pipeline workflow](http://spark.apache.org/docs/latest/ml-pipeline.html) for use with Dataframes.\n\n#### Main Components\n* Transformers: Transform a Dataframe into another Dataframe using .transform(), typically through adding a column.\n* Estimators: An algorithm which can be .fit() onto a Dataframe to produce a Transformer.\n* Pipeline: Object containing stages of Transformers and Estimators.\n\nWe have already used 2 different Transformers and an Estimator to get our categorical feature vector.\n* __StringIndexer__ (Estimator) was used fit to the filteredDF Dataframe to produce a Transformer __indexer__.\n* __indexer__ (Transformer) was used to transform the filteredDF Dataframe to produce a new Dataframe, indexedDF.\n* __OneHotEncoder__ (Transformer) was used to transform indexedDf Dataframe to encodedDF Dataframe."],"metadata":{}},{"cell_type":"code","source":["# Columns containing categorical data\n# Create binary featuers with OneHotEncoder for these columns\ncols = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked']\n\n# Create a list of indexers, 1 per categorical column\nindexers = [\n    StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c))\n    for c in cols\n]\n\n# Create a list of encoders, 1 per indexer\nencoders = [\n    OneHotEncoder(\n        inputCol=indexer.getOutputCol(),\n        outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) \n    for indexer in indexers\n]\n\n# Create singular feature vector starting with numerical columns\n# and appending encoded features\nassembler = VectorAssembler(inputCols=['Age', 'Fare'] + [\n  encoder.getOutputCol() for encoder in encoders], \n                            outputCol=\"features\")\n\n# Create pipeline with all stages\npipeline_stages = indexers + encoders + [assembler]\npipeline = Pipeline(stages=pipeline_stages)\n\n# Create a pipeline model by fitting pipeline to filtered_df\npipeline_model = pipeline.fit(filtered_df)\n\n# Get a new Dataframe with only labels and predictions using pipeline_model to transform filtered_df\nfeatures_label_df = pipeline_model.transform(filtered_df).select(col('features'), col('label'))\n\ndisplay(features_label_df)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["## Step 4: Splitting the Dataframe into training and testing data\nNow that we have a Dataframe with a __features__ and __label__ column, we are ready to split our data into testing and training sets, train a model using the training data, and test the model using the testing data.\nWe will use a 70/30 split for training/testing data. We would like to use as much data as possible to train the model while still having enough testing data to get good prediction statistics on the accuracy of the model."],"metadata":{}},{"cell_type":"code","source":["# Get training and testing data randomly\n(training_df, testing_df) = features_label_df.randomSplit([0.7, 0.3])\nprint(\"%d (%f percent) training data rows\" % (training_df.count(), 100.0*training_df.count()/features_label_df.count()))\nprint(\"%d (%f percent) testing data rows\" % (testing_df.count(), 100.0*testing_df.count()/features_label_df.count()))"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["# Part 5: Train and Test a Logistic Regression Model\n\n## Step 1: Training a Logistic Regression Model\nWe will use [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) for our first classifier.\nLogistic Regression is an algorithm which gives a probability [0,1] that a certain feature vector results in a label of 1.\n\nFor example, predicting if a student passes an exam based purely on a single feature, how many hours a student studies.\n\n![example logistic regression](https://upload.wikimedia.org/wikipedia/commons/6/6d/Exam_pass_logistic_curve.jpeg)"],"metadata":{}},{"cell_type":"code","source":["# Create a Logistic Regression Estimator with default parameters\nlr = LogisticRegression()\n\n# Fit the Logistic Regression Estimator with the training data to create a Logistic Regression Model Transformer\nlr_model = lr.fit(training_df)\n\n# Get Dataframe of predictions using the lr_model Transfomer on the testing data\nlr_predictions_df = lr_model.transform(testing_df)\n\ndisplay(lr_predictions_df)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["## Step 2: Testing the Logistic Regression Model\n\nWe will be using the area under the curve (AUC) for both the Precision-Recall (PR) curve and the Receiver Operating Characteristic (ROC) curve.\n\n__References__:\n* [Precision-Recall curve](https://en.wikipedia.org/wiki/Precision_and_recall)\n* [Receiver Operating Characteristic curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)\n* [Connection between PR and ROC curves](http://pages.cs.wisc.edu/~jdavis/davisgoadrichcamera2.pdf)"],"metadata":{}},{"cell_type":"code","source":["# Create an evaluation object\nlr_evaluator = BinaryClassificationEvaluator()\n\n# Get the AUC-ROC and AUC-PR values for the prediction Dataframe\nlr_AUC_ROC = lr_evaluator.evaluate(lr_predictions_df, {lr_evaluator.metricName: \"areaUnderROC\"})\nlr_AUC_PR = lr_evaluator.evaluate(lr_predictions_df, {lr_evaluator.metricName: \"areaUnderPR\"})\n\nprint(\"Area under the Precision Recall Curve: %f\" % (lr_AUC_PR))\nprint(\"Area under the Receiver Operating Characteristic Curve: %f\" % (lr_AUC_ROC))"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["# Part 6: Train and Test a Decision Tree Model\n\n## Step 1: Training a Decision Tree Model\nWe will use [decision tree](https://en.wikipedia.org/wiki/Decision_tree) for our second classifier.\nA decision tree is an algorithm which gives a hard label, (0 or 1), for a given feature vector.\n\n![example decision tree](https://upload.wikimedia.org/wikipedia/commons/f/f3/CART_tree_titanic_survivors.png)"],"metadata":{}},{"cell_type":"code","source":["# Create a Decision Tree Estimator with default parameters\ndt = DecisionTreeClassifier()\n\n# Fit the Decision Tree Estimator with the training data to create a Decision Tree Model Transformer\ndt_model = dt.fit(training_df)\n\n# Get Dataframe of predictions using the dt_model Transfomer on the testing data\ndt_predictions_df = dt_model.transform(testing_df)\n\ndisplay(dt_predictions_df)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["## Step 2: Testing the Decision Tree Model\n\nWe will be again be using the area under the curve (AUC) for both the Precision-Recall (PR) curve and the Receiver Operating Characteristic (ROC) curve."],"metadata":{}},{"cell_type":"code","source":["# Create an evaluation object\ndt_evaluator = BinaryClassificationEvaluator()\n\n# Get the AUC-ROC and AUC-PR values for the prediction Dataframe\ndt_AUC_ROC = dt_evaluator.evaluate(dt_predictions_df, {dt_evaluator.metricName: \"areaUnderROC\"})\ndt_AUC_PR = dt_evaluator.evaluate(dt_predictions_df, {dt_evaluator.metricName: \"areaUnderPR\"})\n\nprint(\"Area under the Precision Recall Curve: %f\" % (dt_AUC_PR))\nprint(\"Area under the Receiver Operating Characteristic Curve: %f\" % (dt_AUC_ROC))"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["# Part 7: Comparing the Logistic Regression Model to the Decision Tree Model"],"metadata":{}},{"cell_type":"code","source":["lr_sur_perr    = (lr_predictions_df.select(col('label'), col('prediction')).rdd.map(lambda x: (x[0], x[1])).filter(lambda (l,p): ((l==1) & (p==0))).count()*100.0/\n                  lr_predictions_df.where(col('label')==1).count())\nlr_notsur_perr = (lr_predictions_df.select(col('label'), col('prediction')).rdd.map(lambda x: (x[0], x[1])).filter(lambda (l,p): ((l==0) & (p==1))).count()*100.0/\n                  lr_predictions_df.where(col('label')==0).count())\ndt_sur_perr    = (dt_predictions_df.select(col('label'), col('prediction')).rdd.map(lambda x: (x[0], x[1])).filter(lambda (l,p): ((l==1) & (p==0))).count()*100.0/\n                  dt_predictions_df.where(col('label')==1).count())\ndt_notsur_perr = (dt_predictions_df.select(col('label'), col('prediction')).rdd.map(lambda x: (x[0], x[1])).filter(lambda (l,p): ((l==0) & (p==1))).count()*100.0/\n                  dt_predictions_df.where(col('label')==0).count())\n\nprint(\"Logistic Regression\")\nprint(\"  AUC-PR:                        %f\" % (lr_AUC_PR))\nprint(\"  AUC_ROC:                       %f\" % (lr_AUC_ROC))\nprint(\"  Survived percent error:        %f\" % (lr_sur_perr))\nprint(\"  Did not survive percent error: %f\" % (lr_notsur_perr))\nprint(\"\\n\")\nprint(\"Decision Tree\")\nprint(\"  AUC-PR:                        %f\" % (dt_AUC_PR))\nprint(\"  AUC_ROC:                       %f\" % (dt_AUC_ROC))\nprint(\"  Survived percent error:        %f\" % (dt_sur_perr))\nprint(\"  Did not survive percent error: %f\" % (dt_notsur_perr))"],"metadata":{},"outputs":[],"execution_count":34}],"metadata":{"name":"Titanic_Spark","notebookId":2480519669526794},"nbformat":4,"nbformat_minor":0}
