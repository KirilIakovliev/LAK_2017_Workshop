{"cells":[{"cell_type":"markdown","source":["#Titanic Survivors Machine Learning Application\n\n#![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png) + ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png) + ![titanic](https://upload.wikimedia.org/wikipedia/commons/thumb/f/fd/RMS_Titanic_3.jpg/300px-RMS_Titanic_3.jpg)\n\n\n** This notebook covers: **\n* *Part 0: Import Libraries to Use for this Notebook*\n* *Part 1: Import Titanic Dataset as a Spark Dataframe*\n* *Part 2: Choosing Features and Cleaning Data*\n* *Part 3: Working with Categorical Features and Generating Training/Testing Datasets*\n* *Part 4: Train and Test a Logistic Regression Model*\n* *Part 5: Train and Test a Decision Tree Model*\n* *Part 6: Comparing the Logistic Regression Model to the Decision Tree Model*\n\n\n## Dataset Definition\nTitanic dataset from [kaggle](https://www.kaggle.com/c/titanic/data).\n\n### Data Dictionary\n* survival - Survival (0 = No, 1 = Yes)\n* pclass - Ticket class\t(1 = 1st, 2 = 2nd, 3 = 3rd)\n* sex - Sex\t\n* Age - Age in years\t\n* sibsp - # of siblings / spouses aboard the Titanic\t\n* parch\t- # of parents / children aboard the Titanic\t\n* ticket - Ticket number\n* fare - Passenger fare\n* cabin - Cabin number\n* embarked - Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)\n\n#### Variable Notes\n* pclass: A proxy for socio-economic status (SES)\n  * 1st = Upper\n  * 2nd = Middle\n  * 3rd = Lower\n* age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n* sibsp: The dataset defines family relations in this way...\n  * Sibling = brother, sister, stepbrother, stepsister\n  * Spouse = husband, wife (mistresses and fiancÃ©s were ignored)\n* parch: The dataset defines family relations in this way...\n  * Parent = mother, father\n  * Child = daughter, son, stepdaughter, stepson\n  * Some children travelled only with a nanny, therefore parch=0 for them."],"metadata":{}},{"cell_type":"markdown","source":["# Part 0: Import Libraries to use for this Notebook"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col\nfrom pyspark.sql.types import DoubleType, StringType, IntegerType\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer, VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["# Part 1: Import Titanic Dataset as a Spark Dataframe\n\n## Create a table named \"titanic\" from the train.csv file from Kaggle"],"metadata":{}},{"cell_type":"code","source":["# Read data from the newly created titanic table into a dataframe\ntitanic_df = sqlContext.sql(\"SELECT * FROM titanic\")\ndisplay(titanic_df)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["# Part 2: Choosing Features and Cleaning Data\n\n## What are we predicting?\nOur goal for this application will be to predict if a passenger survived the sinking of the Titanic or not given features about the passenger. Our label which we are trying to predict will be the __Survived__ column.\n\n## Feature Columns\n* __Pclass__: Categorical\n* __Sex__: Categorical\n* __Age__: Float\n* __SibSp__: Categorical\n* __Parch__: Categorical\n* __Fare__: Float\n* __Embarked__: Categorical\n\n## Cleaning Data Before Moving On\nThere are null values in the Titanic dataset for some columns we will be using. We will now select only columns we will be using as features or labels and then filter to only keep rows with non-null values. Our new filtered dataframe will be called __filtered_df__.\n\n__Note:__ In a real world application it may be desirable to transform null column values to non-null values depending on the context of the application and the data."],"metadata":{}},{"cell_type":"code","source":["# Select only the columns we will use for features or label\ntmp_df = titanic_df.select(col('Survived').cast(IntegerType()).alias('label'), \n                           col('Pclass').cast(StringType()).alias('Pclass'), \n                           col('Sex').cast(StringType()).alias('Sex'), \n                           col('Age').cast(DoubleType()).alias('Age'),\n                           col('SibSp').cast(StringType()).alias('SibSp'), \n                           col('Parch').cast(StringType()).alias('Parch'), \n                           col('Fare').cast(DoubleType()).alias('Fare'),\n                           col('Embarked').cast(StringType()).alias('Embarked')\n                          )\n\n# Filter any rows from tmp_df that have null values in any of their columns\ng = ((col('label').isNotNull()) & \n     (col('Pclass').isNotNull()) & \n     (col('Sex').isNotNull()) & \n     (col('Age').isNotNull()) & \n     (col('SibSp').isNotNull()) & \n     (col('Parch').isNotNull()) & \n     (col('Fare').isNotNull()) &\n     (col('Embarked').isNotNull())\n    )\nfiltered_df = tmp_df.where(g)\n\nprint(\"%d rows in original dataset\\n%d rows in filtered dataset\\n\\n%d rows filtered with null values\" % (titanic_df.count(), filtered_df.count(), titanic_df.count()-filtered_df.count()))"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["display(filtered_df)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["# Part 3: Working with Categorical Features and Generating Training/Testing Datasets\nCategorical features need special treatment since mathematically the ordering of categories is arbitrary.\nFor example, the __Sex__ column contains values (male and female) which could be converted to (0 and 1) but this is not good!\nBetter to use [one hot encoding](https://en.wikipedia.org/wiki/One-hot) which converts each feature into an _n_ dimensional vector where _n_ is the number of differnt categories for the feature.\n\nTherefore, for the __Sex__ column we can use one hot encoding\n\nmale   -> [0,1]\nfemale -> [1,0]\n\n## One hot encoding in Spark\nLet us now use Spark to apply one hot encoding on the __Sex__ column of data.\n\n### Step 1: Get index from categorical columns\nConvert variables of strings to an index. For example\n\ndata = ['male', 'male', 'female', 'male', 'female']\n\nwould become\n\nindexedData = [0, 0, 1, 0, 1]"],"metadata":{}},{"cell_type":"code","source":["# Create a StringIndexer object for the Sex column and create an indexed Sex_numeric column\nindexer = StringIndexer(inputCol=\"Sex\", outputCol=\"Sex_numeric\").fit(filtered_df)\n\n# Use new indexer object to transform the filtered_df dataframe, adding a new column to it\nindexed_df = indexer.transform(filtered_df)\n\n# Display only the Sex and Sex_numeric columns for clarity\ndisplay(indexed_df.select(col('Sex'), col('Sex_numeric')))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["### Step 2: Get One Hot Encoding Vector for Indexed Categorical Data\n\nWe will now use the indexed categorical data to create a one hot encoding vector for the __Sex__ column features. In Spark, sparse vectors are used which only shows values if they are non-zero in a map type way. \n\nFor example:\n\n[0, 0, 1, 2] -> (0, 2, [2, 3], [1, 2]) -> (first_numeric_value, last_numeric_value, [non-zero_indicies], [non-zero_values])"],"metadata":{}},{"cell_type":"code","source":["# Create a OneHotEncoder object for the Sex_numeric column as Sex_vector column\nencoder = OneHotEncoder(inputCol=\"Sex_numeric\", outputCol=\"Sex_vector\")\n\n# Use new encoder object to transform the indexed_df dataframe, adding a new column to it\nencoded_df = encoder.transform(indexed_df)\n\n# Display only the Sex, Sex_numeric, and Sex_vector columns for clarity\ndisplay(encoded_df.select(col('Sex'), col('Sex_numeric'), col('Sex_vector')))"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["### Step 3: Setup a Pipeline to Encode all Categorical Variables\nSpark offers a [Pipeline workflow](http://spark.apache.org/docs/latest/ml-pipeline.html) for use with Dataframes.\n\n#### Main Components\n* Transformers: Transform a Dataframe into another Dataframe using .transform(), typically through adding a column.\n* Estimators: An algorithm which can be .fit() onto a Dataframe to produce a Transformer.\n* Pipeline: Object containing stages of Transformers and Estimators.\n\nWe have already used 2 different Transformers and an Estimator to get our categorical feature vector.\n* __StringIndexer__ (Estimator) was used fit to the filteredDF Dataframe to produce a Transformer __indexer__.\n* __indexer__ (Transformer) was used to transform the filteredDF Dataframe to produce a new Dataframe, indexedDF.\n* __OneHotEncoder__ (Transformer) was used to transform indexedDf Dataframe to encodedDF Dataframe."],"metadata":{}},{"cell_type":"code","source":["# Columns containing categorical data\n# Create binary featuers with OneHotEncoder for these columns\ncols = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked']\n\n# Create a list of indexers, 1 per categorical column\nindexers = [\n    StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c))\n    for c in cols\n]\n\n# Create a list of encoders, 1 per indexer\nencoders = [\n    OneHotEncoder(\n        inputCol=indexer.getOutputCol(),\n        outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) \n    for indexer in indexers\n]\n\n# Create singular feature vector starting with numerical columns\n# and appending encoded features\nassembler = VectorAssembler(inputCols=['Age', 'Fare'] + [\n  encoder.getOutputCol() for encoder in encoders], \n                            outputCol=\"features\")\n\n# Create pipeline with all stages\npipeline_stages = indexers + encoders + [assembler]\npipeline = Pipeline(stages=pipeline_stages)\n\n# Create a pipeline model by fitting pipeline to filtered_df\npipeline_model = pipeline.fit(filtered_df)\n\n# Get a new Dataframe with only labels and predictions using pipeline_model to transform filtered_df\nfeatures_label_df = pipeline_model.transform(filtered_df).select(col('features'), col('label'))\n\ndisplay(features_label_df)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["## Step 4: Splitting the Dataframe into training and testing data\nNow that we have a Dataframe with a __features__ and __label__ column, we are ready to split our data into testing and training sets, train a model using the training data, and test the model using the testing data.\nWe will use a 70/30 split for training/testing data. We would like to use as much data as possible to train the model while still having enough testing data to get good prediction statistics on the accuracy of the model."],"metadata":{}},{"cell_type":"code","source":["# Get training and testing data randomly\n(training_df, testing_df) = features_label_df.randomSplit([0.7, 0.3])\nprint(\"%d (%f percent) training data rows\" % (training_df.count(), 100.0*training_df.count()/features_label_df.count()))\nprint(\"%d (%f percent) testing data rows\" % (testing_df.count(), 100.0*testing_df.count()/features_label_df.count()))"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["# Part 4: Train and Test a Logistic Regression Model\n\n## Step 1: Training a Logistic Regression Model\nWe will use [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) for our first classifier.\nLogistic Regression is an algorithm which gives a probability [0,1] that a certain feature vector results in a label of 1.\n\nFor example, predicting if a student passes an exam based purely on a single feature, how many hours a student studies.\n\n![example logistic regression](https://upload.wikimedia.org/wikipedia/commons/6/6d/Exam_pass_logistic_curve.jpeg)"],"metadata":{}},{"cell_type":"code","source":["# Create a Logistic Regression Estimator with default parameters\nlr = LogisticRegression()\n\n# Fit the Logistic Regression Estimator with the training data to create a Logistic Regression Model Transformer\nlr_model = lr.fit(training_df)\n\n# Get Dataframe of predictions using the lr_model Transfomer on the testing data\nlr_predictions_df = lr_model.transform(testing_df)\n\ndisplay(lr_predictions_df)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["## Step 2: Testing the Logistic Regression Model\n\nWe will be using the area under the curve (AUC) for both the Precision-Recall (PR) curve and the Receiver Operating Characteristic (ROC) curve.\n\n__References__:\n* [Precision-Recall curve](https://en.wikipedia.org/wiki/Precision_and_recall)\n* [Receiver Operating Characteristic curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)\n* [Connection between PR and ROC curves](http://pages.cs.wisc.edu/~jdavis/davisgoadrichcamera2.pdf)"],"metadata":{}},{"cell_type":"code","source":["# Create an evaluation object\nlr_evaluator = BinaryClassificationEvaluator()\n\n# Get the AUC-ROC and AUC-PR values for the prediction Dataframe\nlr_AUC_ROC = lr_evaluator.evaluate(lr_predictions_df, {lr_evaluator.metricName: \"areaUnderROC\"})\nlr_AUC_PR = lr_evaluator.evaluate(lr_predictions_df, {lr_evaluator.metricName: \"areaUnderPR\"})\n\nprint(\"Area under the Precision Recall Curve: %f\" % (lr_AUC_PR))\nprint(\"Area under the Receiver Operating Characteristic Curve: %f\" % (lr_AUC_ROC))"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["# Part 5: Train and Test a Decision Tree Model\n\n## Step 1: Training a Decision Tree Model\nWe will use [decision tree](https://en.wikipedia.org/wiki/Decision_tree) for our second classifier.\nA decision tree is an algorithm which gives a hard label, (0 or 1), for a given feature vector.\n\n![example decision tree](https://upload.wikimedia.org/wikipedia/commons/f/f3/CART_tree_titanic_survivors.png)"],"metadata":{}},{"cell_type":"code","source":["# Create a Decision Tree Estimator with default parameters\ndt = DecisionTreeClassifier()\n\n# Fit the Decision Tree Estimator with the training data to create a Decision Tree Model Transformer\ndt_model = dt.fit(training_df)\n\n# Get Dataframe of predictions using the dt_model Transfomer on the testing data\ndt_predictions_df = dt_model.transform(testing_df)\n\ndisplay(dt_predictions_df)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["## Step 2: Testing the Decision Tree Model\n\nWe will be again be using the area under the curve (AUC) for both the Precision-Recall (PR) curve and the Receiver Operating Characteristic (ROC) curve."],"metadata":{}},{"cell_type":"code","source":["# Create an evaluation object\ndt_evaluator = BinaryClassificationEvaluator()\n\n# Get the AUC-ROC and AUC-PR values for the prediction Dataframe\ndt_AUC_ROC = dt_evaluator.evaluate(dt_predictions_df, {dt_evaluator.metricName: \"areaUnderROC\"})\ndt_AUC_PR = dt_evaluator.evaluate(dt_predictions_df, {dt_evaluator.metricName: \"areaUnderPR\"})\n\nprint(\"Area under the Precision Recall Curve: %f\" % (dt_AUC_PR))\nprint(\"Area under the Receiver Operating Characteristic Curve: %f\" % (dt_AUC_ROC))"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["# Part 6: Comparing the Logistic Regression Model to the Decision Tree Model"],"metadata":{}},{"cell_type":"code","source":["lr_sur_perr    = (lr_predictions_df.select(col('label'), col('prediction')).rdd.map(lambda x: (x[0], x[1])).filter(lambda (l,p): ((l==1) & (p==1))).count()*100.0/\n                  lr_predictions_df.where(col('label')==1).count())\nlr_notsur_perr = (lr_predictions_df.select(col('label'), col('prediction')).rdd.map(lambda x: (x[0], x[1])).filter(lambda (l,p): ((l==0) & (p==0))).count()*100.0/\n                  lr_predictions_df.where(col('label')==0).count())\ndt_sur_perr    = (dt_predictions_df.select(col('label'), col('prediction')).rdd.map(lambda x: (x[0], x[1])).filter(lambda (l,p): ((l==1) & (p==1))).count()*100.0/\n                  dt_predictions_df.where(col('label')==1).count())\ndt_notsur_perr = (dt_predictions_df.select(col('label'), col('prediction')).rdd.map(lambda x: (x[0], x[1])).filter(lambda (l,p): ((l==0) & (p==0))).count()*100.0/\n                  dt_predictions_df.where(col('label')==0).count())\n\nprint(\"Logistic Regression\")\nprint(\"  AUC-PR:                        %f\" % (lr_AUC_PR))\nprint(\"  AUC_ROC:                       %f\" % (lr_AUC_ROC))\nprint(\"  Survived percent error:        %f\" % (lr_sur_perr))\nprint(\"  Did not survive percent error: %f\" % (lr_notsur_perr))\nprint(\"\\n\")\nprint(\"Decision Tree\")\nprint(\"  AUC-PR:                        %f\" % (dt_AUC_PR))\nprint(\"  AUC_ROC:                       %f\" % (dt_AUC_ROC))\nprint(\"  Survived percent error:        %f\" % (dt_sur_perr))\nprint(\"  Did not survive percent error: %f\" % (dt_notsur_perr))"],"metadata":{},"outputs":[],"execution_count":26}],"metadata":{"name":"Titanic_Spark","notebookId":2480519669526794},"nbformat":4,"nbformat_minor":0}
